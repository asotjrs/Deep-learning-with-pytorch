{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/asotjrs/Deep-learning-with-pytorch/blob/main/experiment_tracking.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#07; Experiment racking \n",
        "machine learning is very experimental.\n",
        "\n",
        "In order to figure out which experiments are worth pursuing , thats where experiment tracking comes in, it helps you to figure out wht doesn' work so you can figure out what does work.\n",
        "\n",
        "In this notebook, we are going to se an example of programatically tracking eperiment"
      ],
      "metadata": {
        "id": "81WgjB1J83Ia"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch,torchvision\n",
        "torch.__version__ ,torchvision.__version__"
      ],
      "metadata": {
        "id": "tahT7iQiLaOq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l5YClIvzWVYc"
      },
      "outputs": [],
      "source": [
        "# Continue with regular imports\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torchvision\n",
        "\n",
        "from torch import nn\n",
        "from torchvision import transforms\n",
        "\n",
        "# Try to get torchinfo, install it if it doesn't work\n",
        "try:\n",
        "    from torchinfo import summary\n",
        "except:\n",
        "    print(\"[INFO] Couldn't find torchinfo... installing it.\")\n",
        "    !pip install -q torchinfo\n",
        "    from torchinfo import summary\n",
        "\n",
        "# Try to import the going_modular directory, download it from GitHub if it doesn't work\n",
        "try:\n",
        "    from going_modular.going_modular import data_setup, engine\n",
        "except:\n",
        "    # Get the going_modular scripts\n",
        "    print(\"[INFO] Couldn't find going_modular scripts... downloading them from GitHub.\")\n",
        "    !git clone https://github.com/mrdbourke/pytorch-deep-learning\n",
        "    !mv pytorch-deep-learning/going_modular .\n",
        "    !rm -rf pytorch-deep-learning\n",
        "    from going_modular.going_modular import data_setup, engine"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rs6eIlUZWyhw"
      },
      "outputs": [],
      "source": [
        "#setup device agnostic code\n",
        "device=\"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6m_UiilYXk0H"
      },
      "outputs": [],
      "source": [
        "# Set seeds\n",
        "def set_seeds(seed: int=42):\n",
        "    \"\"\"Sets random sets for torch operations.\n",
        "\n",
        "    Args:\n",
        "        seed (int, optional): Random seed to set. Defaults to 42.\n",
        "    \"\"\"\n",
        "    # Set the seed for general torch operations\n",
        "    torch.manual_seed(seed)\n",
        "    # Set the seed for CUDA torch operations (ones that happen on the GPU)\n",
        "    torch.cuda.manual_seed(seed)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zrN2szI1X8-N"
      },
      "outputs": [],
      "source": [
        "set_seeds()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.get data\n",
        "\n",
        "- improve upon theresults we've been getting on FoodVision Mini\n",
        "\n",
        "- ufnctionize the code in the previous section to download the zip file\n"
      ],
      "metadata": {
        "id": "a-17r2u5NMit"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import zipfile\n",
        "from pathlib import Path   \n",
        "import requests \n",
        "def download_data(\n",
        "    source:str,\n",
        "    destination:str,\n",
        "    remove_source:bool=True)->Path:\n",
        "\n",
        "    \"\"\"\n",
        "    Downloads a zipped dataset from source and unzipps to destination.\n",
        "    Args:\n",
        "      source(str): a link to a zipped file containing the data.\n",
        "      destination(str): a target directory to unzip data to \n",
        "      remove_surce(bool): whether to remove source after download or not\n",
        "    Returns:\n",
        "      pathlib.Path to the downloaded data\n",
        "\n",
        "    Example Usage:\n",
        "      download_data(source=\"https://github.com/mrdbourke/pytorch-deep-learning/raw/main/data/pizza_steak_sushi.zip\",\n",
        "                      destination=\"pizza_steak_sushi\")\n",
        "    \"\"\"\n",
        "    #setup path to data folder\n",
        "    data_path=Path(\"data/\")\n",
        "    image_path=data_path/destination \n",
        "    #if theimage folder doesn't exist , download it and prepare it...\n",
        "    if image_path.is_dir(): \n",
        "      print(f'[INFO] {image_path} directory already exists , skipping download ...')\n",
        "    else:\n",
        "      print(f\"[INFO] didn't find {image_path} directory , creating one  \")\n",
        "      image_path.mkdir(parents=True,exist_ok=True)\n",
        "      #download the pizz  a steak sushi data\n",
        "      target_file=Path(source).name\n",
        "      with open(data_path/target_file,'wb') as f:\n",
        "        request=requests.get(source)\n",
        "        print(f'[INFO] Downloading {target_file} from {source}...')\n",
        "        f.write(request.content)\n",
        "      \n",
        "      #unzip pizza, steak suqhi data\n",
        "      with zipfile.ZipFile(data_path/target_file,\"r\") as zip_ref:\n",
        "        print(f'[INFO] Unzipping {target_file} data..')\n",
        "        zip_ref.extractall(image_path)\n",
        "\n",
        "      #remove the .zip file\n",
        "      if remove_source:\n",
        "        os.remove(data_path/target_file)\n",
        "    return image_path\n",
        "\n",
        "image_path = download_data(source=\"https://github.com/mrdbourke/pytorch-deep-learning/raw/main/data/pizza_steak_sushi.zip\",\n",
        "                           destination=\"pizza_steak_sushi\")\n",
        "image_path"
      ],
      "metadata": {
        "id": "fvVjOCe-QDBG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##2.1 Create Dataloaders using maually created transforms"
      ],
      "metadata": {
        "id": "pjxKWLTQ8GyI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#setup directories\n",
        "train_dir=image_path/\"train\"\n",
        "test_dir=image_path/\"test\"\n",
        "# Setup ImageNet normalization levels (turns all images into similar distribution as ImageNet)\n",
        "normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                                 std=[0.229, 0.224, 0.225])\n",
        "#Create  transform pipeline manually\n",
        "manual_transforms=transforms.Compose([\n",
        "    transforms.Resize((224,224)),\n",
        "    transforms.ToTensor(),\n",
        "    normalize])\n",
        "print(f\"Manually created transforms: {manual_transforms}\")\n",
        "#Create data_loaders\n",
        "train_dataloader,test_dataloader, class_names= data_setup.create_dataloaders(\n",
        "    train_dir,\n",
        "    test_dir,\n",
        "    transform=manual_transforms, #use manually created transforms\n",
        "    batch_size=32\n",
        "    )\n",
        "train_dataloader, test_dataloader,class_names"
      ],
      "metadata": {
        "id": "nzU4Et6FusR4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##2.1 Create dataloaders using automatically created transforms\n",
        "\n",
        "- first we need to instanciate a set of pre-trained weights we'd like to use and calling thetransform method on it "
      ],
      "metadata": {
        "id": "5PLfQmvEJQpH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#setup dirs\n",
        "train_dir =image_path/\"train\"\n",
        "test_dir =image_path/\"test\"\n",
        "#setup pre-trained weights (plenty of these are available in torchvision.models)\n",
        "weights=torchvision.models.EfficientNet_B0_Weights.DEFAULT\n",
        "\n",
        "#get transforms from weights(these are the transforms that were used to optain the weights)\n",
        "automatic_transforms=weights.transforms()\n",
        "print(f\"Automatically created transforms: { automatic_transforms}\")\n",
        "\n",
        "#create DataLoaders\n",
        "train_data_loader, test_dataloader, class_names =data_setup.create_dataloaders(\n",
        "    train_dir,\n",
        "    test_dir,\n",
        "    transform=automatic_transforms, #use automatic created transforms\n",
        "    batch_size=32)\n",
        "\n",
        "train_dataloader, test_dataloader, class_names"
      ],
      "metadata": {
        "id": "xOyel3hw_1Tc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##3. Getting a preTrained model , freezing the base layer an changing the classifier head\n"
      ],
      "metadata": {
        "id": "1OKR59ZeJdiO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Note: this is how a pre-trained model will be created  in torchvision>0.13 it will be deprecated in future version\n",
        "#model= torchvision.models.efficientnet_b0(pretrained=True).to(device) #OLD\n",
        "#Download the pre-trained weights for efficientNet_B0\n",
        "weights=torchvision.models.EfficientNet_B0_Weights.DEFAULT # DEFAULT means best available\n",
        "\n",
        "#setup the model with the prerained weights and send it to the target device\n",
        "model=torchvision.models.efficientnet_b0(weights).to(device)\n",
        "\n",
        "#view the output of the model\n",
        "#model\n"
      ],
      "metadata": {
        "id": "TrURlk07M9VJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#freeze all base layers by setting requires_grad attribute to False\n",
        "for param in model.features.parameters():\n",
        "  param.requires_grad=False\n",
        "\n",
        "#since we are creating a new layer with random weights (torch.nn.linear),\n",
        "set_seeds()\n",
        "\n",
        "#Update the classifier head tosuite our problem\n",
        "model.classifier =torch.nn.Sequential(\n",
        "    nn.Dropout(p=0.2,inplace=True),\n",
        "    nn.Linear(in_features=1280,\n",
        "              out_features=len(class_names),\n",
        "              bias=True)).to(device)"
      ],
      "metadata": {
        "id": "LPSl6WbRKbrR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torchinfo import summary\n",
        "#get summary of the model\n",
        "# summary(model,\n",
        "#         input_size=(32,3,244,244), #make sure this is the input_size not the input shape\n",
        "#         verbose=0,\n",
        "#         col_names =[\"input_size\",\"output_size\",\"num_params\",\"trainable\"],\n",
        "#         col_width=20,\n",
        "#         row_settings=[\"var_names\"]\n",
        "#         )"
      ],
      "metadata": {
        "id": "Ir4M2JMxO4Ap"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Train model and track results"
      ],
      "metadata": {
        "id": "L5Ccuq7iaWAR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#define loss and optimizer\n",
        "loss_fn=nn.CrossEntropyLoss()\n",
        "optimizer= torch.optim.Adam(model.parameters(),lr=0.001)  "
      ],
      "metadata": {
        "id": "HD-FecujQLn7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Adjust `train()` function to track results with `SummaryWriter()`"
      ],
      "metadata": {
        "id": "DElYwo9za1Jt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.tensorboard import SummaryWriter\n",
        "#create a writer with all default settings\n",
        "writer = SummaryWriter()\n"
      ],
      "metadata": {
        "id": "dcuLXkbdbDiJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Dict, List\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "from going_modular.going_modular.engine import train_step, test_step\n",
        "\n",
        "# Import train() function from: \n",
        "# https://github.com/mrdbourke/pytorch-deep-learning/blob/main/going_modular/going_modular/engine.py\n",
        "def train(model: torch.nn.Module, \n",
        "          train_dataloader: torch.utils.data.DataLoader, \n",
        "          test_dataloader: torch.utils.data.DataLoader, \n",
        "          optimizer: torch.optim.Optimizer,\n",
        "          loss_fn: torch.nn.Module,\n",
        "          epochs: int,\n",
        "          device: torch.device) -> Dict[str, List]:\n",
        "    \"\"\"Trains and tests a PyTorch model.\n",
        "\n",
        "    Passes a target PyTorch models through train_step() and test_step()\n",
        "    functions for a number of epochs, training and testing the model\n",
        "    in the same epoch loop.\n",
        "\n",
        "    Calculates, prints and stores evaluation metrics throughout.\n",
        "\n",
        "    Args:\n",
        "      model: A PyTorch model to be trained and tested.\n",
        "      train_dataloader: A DataLoader instance for the model to be trained on.\n",
        "      test_dataloader: A DataLoader instance for the model to be tested on.\n",
        "      optimizer: A PyTorch optimizer to help minimize the loss function.\n",
        "      loss_fn: A PyTorch loss function to calculate loss on both datasets.\n",
        "      epochs: An integer indicating how many epochs to train for.\n",
        "      device: A target device to compute on (e.g. \"cuda\" or \"cpu\").\n",
        "      \n",
        "    Returns:\n",
        "      A dictionary of training and testing loss as well as training and\n",
        "      testing accuracy metrics. Each metric has a value in a list for \n",
        "      each epoch.\n",
        "      In the form: {train_loss: [...],\n",
        "                train_acc: [...],\n",
        "                test_loss: [...],\n",
        "                test_acc: [...]} \n",
        "      For example if training for epochs=2: \n",
        "              {train_loss: [2.0616, 1.0537],\n",
        "                train_acc: [0.3945, 0.3945],\n",
        "                test_loss: [1.2641, 1.5706],\n",
        "                test_acc: [0.3400, 0.2973]} \n",
        "    \"\"\"\n",
        "    # Create empty results dictionary\n",
        "    results = {\"train_loss\": [],\n",
        "               \"train_acc\": [],\n",
        "               \"test_loss\": [],\n",
        "               \"test_acc\": []\n",
        "    }\n",
        "\n",
        "    # Loop through training and testing steps for a number of epochs\n",
        "    for epoch in tqdm(range(epochs)):\n",
        "        train_loss, train_acc = train_step(model=model,\n",
        "                                           dataloader=train_dataloader,\n",
        "                                           loss_fn=loss_fn,\n",
        "                                           optimizer=optimizer,\n",
        "                                           device=device)\n",
        "        test_loss, test_acc = test_step(model=model,\n",
        "                                        dataloader=test_dataloader,\n",
        "                                        loss_fn=loss_fn,\n",
        "                                        device=device)\n",
        "\n",
        "        # Print out what's happening\n",
        "        print(\n",
        "          f\"Epoch: {epoch+1} | \"\n",
        "          f\"train_loss: {train_loss:.4f} | \"\n",
        "          f\"train_acc: {train_acc:.4f} | \"\n",
        "          f\"test_loss: {test_loss:.4f} | \"\n",
        "          f\"test_acc: {test_acc:.4f}\"\n",
        "        )\n",
        "\n",
        "        # Update results dictionary\n",
        "        results[\"train_loss\"].append(train_loss)\n",
        "        results[\"train_acc\"].append(train_acc)\n",
        "        results[\"test_loss\"].append(test_loss)\n",
        "        results[\"test_acc\"].append(test_acc)\n",
        "\n",
        "        ### New: Experiment tracking ###\n",
        "        # Add loss results to SummaryWriter\n",
        "        writer.add_scalars(main_tag=\"Loss\", \n",
        "                           tag_scalar_dict={\"train_loss\": train_loss,\n",
        "                                            \"test_loss\": test_loss},\n",
        "                           global_step=epoch)\n",
        "\n",
        "        # Add accuracy results to SummaryWriter\n",
        "        writer.add_scalars(main_tag=\"Accuracy\", \n",
        "                           tag_scalar_dict={\"train_acc\": train_acc,\n",
        "                                            \"test_acc\": test_acc}, \n",
        "                           global_step=epoch)\n",
        "        \n",
        "        # Track the PyTorch model architecture\n",
        "        writer.add_graph(model=model, \n",
        "                         # Pass in an example input\n",
        "                         input_to_model=torch.randn(32, 3, 224, 224).to(device))\n",
        "    \n",
        "    # Close the writer\n",
        "    writer.close()\n",
        "    \n",
        "    ### End new ###\n",
        "\n",
        "    # Return the filled results at the end of the epochs\n",
        "    return results"
      ],
      "metadata": {
        "id": "T7F3rswnAgfA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "set_seeds()\n",
        "results = train(model,train_dataloader, test_dataloader, optimizer,loss_fn,5,device)\n"
      ],
      "metadata": {
        "id": "Dqy5QwBWCTfm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from torch.utils import tensorboard\n",
        "# %load_ext tensorboard\n",
        "# %tensorboard --logdir runs\n",
        "results"
      ],
      "metadata": {
        "id": "hDOFTjHbDaLI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_writer(experiment_name: str, \n",
        "                  model_name: str, \n",
        "                  extra: str=None) -> torch.utils.tensorboard.writer.SummaryWriter():\n",
        "    \"\"\"Creates a torch.utils.tensorboard.writer.SummaryWriter() instance saving to a specific log_dir.\n",
        "\n",
        "    log_dir is a combination of runs/timestamp/experiment_name/model_name/extra.\n",
        "\n",
        "    Where timestamp is the current date in YYYY-MM-DD format.\n",
        "\n",
        "    Args:\n",
        "        experiment_name (str): Name of experiment.\n",
        "        model_name (str): Name of model.\n",
        "        extra (str, optional): Anything extra to add to the directory. Defaults to None.\n",
        "\n",
        "    Returns:\n",
        "        torch.utils.tensorboard.writer.SummaryWriter(): Instance of a writer saving to log_dir.\n",
        "\n",
        "    Example usage:\n",
        "        # Create a writer saving to \"runs/2022-06-04/data_10_percent/effnetb2/5_epochs/\"\n",
        "        writer = create_writer(experiment_name=\"data_10_percent\",\n",
        "                               model_name=\"effnetb2\",\n",
        "                               extra=\"5_epochs\")\n",
        "        # The above is the same as:\n",
        "        writer = SummaryWriter(log_dir=\"runs/2022-06-04/data_10_percent/effnetb2/5_epochs/\")\n",
        "    \"\"\"\n",
        "    from datetime import datetime\n",
        "    import os\n",
        "\n",
        "    # Get timestamp of current date (all experiments on certain day live in same folder)\n",
        "    timestamp = datetime.now().strftime(\"%Y-%m-%d\") # returns current date in YYYY-MM-DD format\n",
        "\n",
        "    if extra:\n",
        "        # Create log directory path\n",
        "        log_dir = os.path.join(\"runs\", timestamp, experiment_name, model_name, extra)\n",
        "    else:\n",
        "        log_dir = os.path.join(\"runs\", timestamp, experiment_name, model_name)\n",
        "        \n",
        "    print(f\"[INFO] Created SummaryWriter, saving to: {log_dir}...\")\n",
        "    return SummaryWriter(log_dir=log_dir)"
      ],
      "metadata": {
        "id": "oSTGi5UvFUvN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create an example writer\n",
        "example_writer = create_writer(experiment_name=\"data_10_percent\",\n",
        "                               model_name=\"effnetb0\",\n",
        "                               extra=\"5_epochs\")"
      ],
      "metadata": {
        "id": "7CHbFGQlarfD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Dict, List\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "# Add writer parameter to train()\n",
        "def train(model: torch.nn.Module, \n",
        "          train_dataloader: torch.utils.data.DataLoader, \n",
        "          test_dataloader: torch.utils.data.DataLoader, \n",
        "          optimizer: torch.optim.Optimizer,\n",
        "          loss_fn: torch.nn.Module,\n",
        "          epochs: int,\n",
        "          device: torch.device, \n",
        "          writer: torch.utils.tensorboard.writer.SummaryWriter # new parameter to take in a writer\n",
        "          ) -> Dict[str, List]:\n",
        "    \"\"\"Trains and tests a PyTorch model.\n",
        "\n",
        "    Passes a target PyTorch models through train_step() and test_step()\n",
        "    functions for a number of epochs, training and testing the model\n",
        "    in the same epoch loop.\n",
        "\n",
        "    Calculates, prints and stores evaluation metrics throughout.\n",
        "\n",
        "    Stores metrics to specified writer log_dir if present.\n",
        "\n",
        "    Args:\n",
        "      model: A PyTorch model to be trained and tested.\n",
        "      train_dataloader: A DataLoader instance for the model to be trained on.\n",
        "      test_dataloader: A DataLoader instance for the model to be tested on.\n",
        "      optimizer: A PyTorch optimizer to help minimize the loss function.\n",
        "      loss_fn: A PyTorch loss function to calculate loss on both datasets.\n",
        "      epochs: An integer indicating how many epochs to train for.\n",
        "      device: A target device to compute on (e.g. \"cuda\" or \"cpu\").\n",
        "      writer: A SummaryWriter() instance to log model results to.\n",
        "\n",
        "    Returns:\n",
        "      A dictionary of training and testing loss as well as training and\n",
        "      testing accuracy metrics. Each metric has a value in a list for \n",
        "      each epoch.\n",
        "      In the form: {train_loss: [...],\n",
        "                train_acc: [...],\n",
        "                test_loss: [...],\n",
        "                test_acc: [...]} \n",
        "      For example if training for epochs=2: \n",
        "              {train_loss: [2.0616, 1.0537],\n",
        "                train_acc: [0.3945, 0.3945],\n",
        "                test_loss: [1.2641, 1.5706],\n",
        "                test_acc: [0.3400, 0.2973]} \n",
        "    \"\"\"\n",
        "    # Create empty results dictionary\n",
        "    results = {\"train_loss\": [],\n",
        "               \"train_acc\": [],\n",
        "               \"test_loss\": [],\n",
        "               \"test_acc\": []\n",
        "    }\n",
        "\n",
        "    # Loop through training and testing steps for a number of epochs\n",
        "    for epoch in tqdm(range(epochs)):\n",
        "        train_loss, train_acc = train_step(model=model,\n",
        "                                          dataloader=train_dataloader,\n",
        "                                          loss_fn=loss_fn,\n",
        "                                          optimizer=optimizer,\n",
        "                                          device=device)\n",
        "        test_loss, test_acc = test_step(model=model,\n",
        "          dataloader=test_dataloader,\n",
        "          loss_fn=loss_fn,\n",
        "          device=device)\n",
        "\n",
        "        # Print out what's happening\n",
        "        print(\n",
        "          f\"Epoch: {epoch+1} | \"\n",
        "          f\"train_loss: {train_loss:.4f} | \"\n",
        "          f\"train_acc: {train_acc:.4f} | \"\n",
        "          f\"test_loss: {test_loss:.4f} | \"\n",
        "          f\"test_acc: {test_acc:.4f}\"\n",
        "        )\n",
        "\n",
        "        # Update results dictionary\n",
        "        results[\"train_loss\"].append(train_loss)\n",
        "        results[\"train_acc\"].append(train_acc)\n",
        "        results[\"test_loss\"].append(test_loss)\n",
        "        results[\"test_acc\"].append(test_acc)\n",
        "\n",
        "\n",
        "        ### New: Use the writer parameter to track experiments ###\n",
        "        # See if there's a writer, if so, log to it\n",
        "        if writer:\n",
        "            # Add results to SummaryWriter\n",
        "            writer.add_scalars(main_tag=\"Loss\", \n",
        "                               tag_scalar_dict={\"train_loss\": train_loss,\n",
        "                                                \"test_loss\": test_loss},\n",
        "                               global_step=epoch)\n",
        "            writer.add_scalars(main_tag=\"Accuracy\", \n",
        "                               tag_scalar_dict={\"train_acc\": train_acc,\n",
        "                                                \"test_acc\": test_acc}, \n",
        "                               global_step=epoch)\n",
        "\n",
        "            # Close the writer\n",
        "            writer.close()\n",
        "        else:\n",
        "            pass\n",
        "    ### End new ###\n",
        "\n",
        "    # Return the filled results at the end of the epochs\n",
        "    return results"
      ],
      "metadata": {
        "id": "iD07nWofax4C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Download 10 percent and 20 percent training data (if necessary)\n",
        "data_10_percent_path = download_data(source=\"https://github.com/mrdbourke/pytorch-deep-learning/raw/main/data/pizza_steak_sushi.zip\",\n",
        "                                     destination=\"pizza_steak_sushi\")\n",
        "\n",
        "data_20_percent_path = download_data(source=\"https://github.com/mrdbourke/pytorch-deep-learning/raw/main/data/pizza_steak_sushi_20_percent.zip\",\n",
        "                                     destination=\"pizza_steak_sushi_20_percent\")"
      ],
      "metadata": {
        "id": "zLO65evna97j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Setup training directory paths\n",
        "train_dir_10_percent = data_10_percent_path / \"train\"\n",
        "train_dir_20_percent = data_20_percent_path / \"train\"\n",
        "\n",
        "# Setup testing directory paths (note: use the same test dataset for both to compare the results)\n",
        "test_dir = data_10_percent_path / \"test\"\n",
        "\n",
        "# Check the directories\n",
        "print(f\"Training directory 10%: {train_dir_10_percent}\")\n",
        "print(f\"Training directory 20%: {train_dir_20_percent}\")\n",
        "print(f\"Testing directory: {test_dir}\")"
      ],
      "metadata": {
        "id": "hJl5Pb8CDxNs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision import transforms\n",
        "\n",
        "# Create a transform to normalize data distribution to be inline with ImageNet\n",
        "normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406], # values per colour channel [red, green, blue]\n",
        "                                 std=[0.229, 0.224, 0.225]) # values per colour channel [red, green, blue]\n",
        "\n",
        "# Compose transforms into a pipeline\n",
        "simple_transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)), # 1. Resize the images\n",
        "    transforms.ToTensor(), # 2. Turn the images into tensors with values between 0 & 1\n",
        "    normalize # 3. Normalize the images so their distributions match the ImageNet dataset \n",
        "])"
      ],
      "metadata": {
        "id": "wURaBY1TFwUh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 32\n",
        "\n",
        "# Create 10% training and test DataLoaders\n",
        "train_dataloader_10_percent, test_dataloader, class_names = data_setup.create_dataloaders(train_dir=train_dir_10_percent,\n",
        "    test_dir=test_dir, \n",
        "    transform=simple_transform,\n",
        "    batch_size=BATCH_SIZE\n",
        ")\n",
        "\n",
        "# Create 20% training and test data DataLoders\n",
        "train_dataloader_20_percent, test_dataloader, class_names = data_setup.create_dataloaders(train_dir=train_dir_20_percent,\n",
        "    test_dir=test_dir,\n",
        "    transform=simple_transform,\n",
        "    batch_size=BATCH_SIZE\n",
        ")\n",
        "\n",
        "# Find the number of samples/batches per dataloader (using the same test_dataloader for both experiments)\n",
        "print(f\"Number of batches of size {BATCH_SIZE} in 10 percent training data: {len(train_dataloader_10_percent)}\")\n",
        "print(f\"Number of batches of size {BATCH_SIZE} in 20 percent training data: {len(train_dataloader_20_percent)}\")\n",
        "print(f\"Number of batches of size {BATCH_SIZE} in testing data: {len(train_dataloader_10_percent)} (all experiments will use the same test set)\")\n",
        "print(f\"Number of classes: {len(class_names)}, class names: {class_names}\")"
      ],
      "metadata": {
        "id": "niAM8rVuHikq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torchvision\n",
        "from torchinfo import summary\n",
        "\n",
        "# 1. Create an instance of EffNetB2 with pretrained weights\n",
        "effnetb2_weights = torchvision.models.EfficientNet_B2_Weights.DEFAULT # \"DEFAULT\" means best available weights\n",
        "effnetb2 = torchvision.models.efficientnet_b2(weights=effnetb2_weights)\n",
        "\n",
        "# # 2. Get a summary of standard EffNetB2 from torchvision.models (uncomment for full output)\n",
        "# summary(model=effnetb2, \n",
        "#         input_size=(32, 3, 224, 224), # make sure this is \"input_size\", not \"input_shape\"\n",
        "#         # col_names=[\"input_size\"], # uncomment for smaller output\n",
        "#         col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n",
        "#         col_width=20,\n",
        "#         row_settings=[\"var_names\"]\n",
        "# ) \n",
        "\n",
        "# 3. Get the number of in_features of the EfficientNetB2 classifier layer\n",
        "print(f\"Number of in_features to final layer of EfficientNetB2: {len(effnetb2.classifier.state_dict()['1.weight'][0])}\")"
      ],
      "metadata": {
        "id": "C9aVnq7_H-8e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torchvision \n",
        "from torch import nn \n",
        "#get the mumber out of our classes (pizza steak sushi)\n",
        "OUT_FEATURES=len(class_names)\n",
        "#create an EffNetB0 feature extractor\n",
        "def create_effnetb0():\n",
        "  #get the base model with the pre trainedweights and send it to the target device\n",
        "  weights=torchvision.models.EfficientNet_B0_Weights.DEFAULT\n",
        "  model= torchvision.models.efficientnet_b0(weights).to(device)\n",
        "\n",
        "  #2.freeze the base model layers\n",
        "  for param in model.features.parameters():\n",
        "    param.requires_grad=False\n",
        "\n",
        "  #set the seed\n",
        "  set_seeds()\n",
        "\n",
        "  #4 change the classifier head\n",
        "  model.classifier =nn.Sequential(\n",
        "      nn.Dropout(p=0.2),\n",
        "      nn.Linear(in_features=1280,out_features=OUT_FEATURES)).to(device)\n",
        "  #5. give the model a name\n",
        "  model.name= \"effnetb0\"\n",
        "  print(f'[INFO] Created new : {model.name} model.')\n",
        "  return model\n",
        "\n",
        "#Create an EffNetB2 feature extractor\n",
        "def create_effnetb2():\n",
        "  #1. get the base model with pretrained weights and send it to the target device\n",
        "  weights=torchvision.models.EfficientNet_B2_Weights.DEFAULT\n",
        "  model=torchvision.models.efficientnet_b2(weights).to(device)\n",
        "\n",
        "  #freeze the base model layer\n",
        "  for param in model.features.parameters():\n",
        "    param.requires_grad=False\n",
        "  \n",
        "  #set the seeds\n",
        "  set_seeds()\n",
        "\n",
        "  #change the classifier head\n",
        "  model.classifier=nn.Sequential(\n",
        "      nn.Dropout(p=0.3),\n",
        "      nn.Linear(in_features=1408, out_features=OUT_FEATURES)\n",
        "  ).to(device)\n",
        "\n",
        "  #give the model a name\n",
        "  model.name=\"effnetb2\"\n",
        "  print(f\"[INFO] Created new {model.name} model. \")\n",
        "  return model\n"
      ],
      "metadata": {
        "id": "u3IuVdKSNLQS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torchinfo import summary\n",
        "effnetb0=create_effnetb0()\n",
        "effnetb2=create_effnetb2()\n",
        "#get an output summary of the layers\n",
        "# summary(\n",
        "#     model=effnetb2,\n",
        "#     input_size=(32,3,244,244),\n",
        "#     col_names=[\"input_size\"],\n",
        "#     col_width=20,\n",
        "#     row_settings=[\"var_names\"]\n",
        "\n",
        "# )"
      ],
      "metadata": {
        "id": "eyUlfXDX1rqj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#create epochs list\n",
        "num_epochs =[5,10]\n",
        "\n",
        "#2. create models list, model for each expriment\n",
        "models=[\"effnetb0\",\"effnetb2\"]\n",
        "\n",
        "#3 create dataloaders dictionary for various dataloader\n",
        "train_dataloaders={\n",
        "    \"data_10_percent\":train_dataloader_10_percent,\n",
        "    \"data_20_percent\":train_dataloader_20_percent}\n",
        "\n",
        "    "
      ],
      "metadata": {
        "id": "XFv1oJyD-CGN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "from going_modular.going_modular.utils import save_model\n",
        "#set the random seeds\n",
        "set_seeds(seed=42)\n",
        "#keep track of th experiment number\n",
        "experiment_number=0\n",
        "#loop through each dataloader\n",
        "for dataloader_name, train_datalaoder in train_dataloaders.items():\n",
        "  #loop througheach of the epochs\n",
        "  for epochs in num_epochs:\n",
        "    #loop through each model name and create a new model based on that name\n",
        "    for model_name in models:\n",
        "      #create information to print out\n",
        "      experiment_number+=1\n",
        "      print(f'[INFO] Experiment number : {experiment_number}')\n",
        "      print(f'[INFO]model: {model_name}')\n",
        "      print(f'[INFO] DataLoader: {dataloader_name} ')\n",
        "      print(f'[INFO] Number of Epochs: {epochs}')\n",
        "\n",
        "      #select the model\n",
        "      if model_name==\"effnetb0\":\n",
        "        model=create_effnetb0()\n",
        "      else:\n",
        "        model=create_effnetb2()\n",
        "      \n",
        "      #create a loss fn and an optimizer\n",
        "      loss_fn=nn.CrossEntropyLoss()\n",
        "      optimizer=torch.optim.Adam(params=model.parameters(),lr=0.001)\n",
        "\n",
        "      #train target model with target dataloader and track experiments\n",
        "      train(model,train_dataloader, test_dataloader, optimizer, loss_fn,epochs,device,\n",
        "            create_writer(dataloader_name,model_name,f'{epochs}_epochs'))\n",
        "      \n",
        "      #save the model to a file so we can get back the best model\n",
        "      save_filepath=f\"07_{model_name}_{dataloader_name}_{epochs}_epochs.pth\"\n",
        "      save_model(model, \"models\",save_filepath)\n",
        "\n",
        "      print(\"-\"*50+\"/n\")\n"
      ],
      "metadata": {
        "id": "J31ShPayDT9C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "yjBYvqGdVytW"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "private_outputs": true,
      "authorship_tag": "ABX9TyPz4QbZQrDcpo+qinHaVpoD",
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
